{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SalvatoreTucci/CPV/blob/main/CPV_Text_categorization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dvrJarQK5xcH"
      },
      "outputs": [],
      "source": [
        "!pip install -U spaCy\n",
        "!python -m spacy download it_core_news_sm"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Ein6QYnp2lsB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4KzoraJ_tYxU"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import string\n",
        "import spacy\n",
        "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
        "from sklearn.base import TransformerMixin\n",
        "from sklearn.pipeline import Pipeline\n",
        "from spacy.lang.it.stop_words import STOP_WORDS\n",
        "from spacy.lang.it import Italian\n",
        "from sklearn import metrics\n",
        "from sklearn import svm\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn import neural_network\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "#init spaCy\n",
        "punctuations = string.punctuation\n",
        "nlp = spacy.load(\"it_core_news_sm\")\n",
        "stop_words = spacy.lang.it.stop_words.STOP_WORDS\n",
        "parser = Italian()\n",
        "\n",
        "# Custom transformer using spaCy\n",
        "class predictors(TransformerMixin):\n",
        "    def transform(self, X, **transform_params):\n",
        "        # Cleaning Text\n",
        "        return [clean_text(text) for text in X]\n",
        "\n",
        "    def fit(self, X, y=None, **fit_params):\n",
        "        return self\n",
        "\n",
        "    def get_params(self, deep=True):\n",
        "        return {}\n",
        "\n",
        "# Basic function to clean the text\n",
        "def clean_text(text):\n",
        "    # Removing spaces and converting text into lowercase\n",
        "    return text.strip().lower()\n",
        "\n",
        "# Tokenizer function\n",
        "def spacy_tokenizer(sentence):\n",
        "    mytokens = parser(sentence)\n",
        "    mytokens = [ word.text for word in mytokens ]\n",
        "    # remove stop words\n",
        "    mytokens = [ word for word in mytokens if word not in stop_words and word not in punctuations ]\n",
        "    # return preprocessed list of tokens\n",
        "    return mytokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AaLbs6-0MeVG"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HAmWRu_E6U2v"
      },
      "outputs": [],
      "source": [
        "import collections\n",
        "\n",
        "df1 = pd.read_csv('/content/drive/MyDrive/cpv.csv', sep=\",\", names=[\"cig\",\"oggetto_bando\",\"oggetto_lotto\",\"oggetto_principale\",\"cpv\",\"desc_cpv\"], encoding='utf-8')\n",
        "df2 = pd.read_csv('/content/drive/MyDrive/cpv_2008.csv', sep=\",\")\n",
        "df2 = df2.rename({'cod_cpv':'cpv'}, axis=1)\n",
        "cpv1 = df1[\"cpv\"]\n",
        "cpv2 = df2[\"cpv\"]\n",
        "\n",
        "# join tra dataset cpv e dataset bandi di gara\n",
        "df3 = pd.merge(cpv2,cpv1,on='cpv', how='left')\n",
        "cpv3 = df3[\"cpv\"]\n",
        "\n",
        "\"\"\"\n",
        "# divisioni\n",
        "y = [x[0:2] for x in cpv1]\n",
        "print(collections.Counter(y))\n",
        "print()\n",
        "\n",
        "# gruppi\n",
        "y = [x[0:3] for x in cpv1]\n",
        "print((collections.Counter(y)))\n",
        "print()\n",
        "\n",
        "# classi\n",
        "y = [x[0:4] for x in cpv1]\n",
        "print(collections.Counter(y))\n",
        "print()\n",
        "\n",
        "# categorie\n",
        "y = [x[0:5] for x in cpv1]\n",
        "print(collections.Counter(y))\n",
        "print()\n",
        "\n",
        "# cpv\n",
        "y = [x[0:10] for x in cpv1]\n",
        "print(len(collections.Counter(y))) # totale elementi supera totale cpv esistenti -> ci sono cpv inesistenti len(y)\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qGtAUjAjwyC6"
      },
      "outputs": [],
      "source": [
        "df1 = df1[df1.duplicated('cpv',keep=False)] # rimuovo i cpv con un solo bando associato\n",
        "\n",
        "X = df1['oggetto_bando']\n",
        "y = df1['cpv'] # the labels\n",
        "X_main, X_test, y_main, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y) # divisione in main e test 80% - 20%\n",
        "print(X_main.shape)\n",
        "print(X_test.shape)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_main, y_main, test_size=0.125, random_state=42, stratify=y_main) # divisione del main in train e valutation 70% - 10%\n",
        "print(X_train.shape)\n",
        "print(X_val.shape)\n",
        "print()\n",
        "print(y_main.value_counts(normalize=True)*100)\n",
        "print(y_test.value_counts(normalize=True)*100)\n",
        "print()\n",
        "print(len(collections.Counter(y))) #10615 9612 7208009 7207006"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PBsuo-gsA1_4"
      },
      "outputs": [],
      "source": [
        "# divisioni\n",
        "y = [x[0:2] for x in df1['cpv']]\n",
        "X_main, X_test, y_main, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y) # divisione in main e test 80% - 20%\n",
        "print(X_main.shape)\n",
        "print(X_test.shape)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_main, y_main, test_size=0.125, random_state=42, stratify=y_main) # divisione del main in train e valutation 70% - 10%\n",
        "print(X_train.shape)\n",
        "print(X_val.shape)\n",
        "print()\n",
        "print(y_main.value_counts(normalize=True)*100)\n",
        "print(y_test(normalize=True)*100)\n",
        "print()\n",
        "\n",
        "# gruppi\n",
        "y = [x[0:3] for x in df1['cpv']]\n",
        "X_main, X_test, y_main, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y) # divisione in main e test 80% - 20%\n",
        "print(X_main.shape)\n",
        "print(X_test.shape)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_main, y_main, test_size=0.125, random_state=42, stratify=y_main) # divisione del main in train e valutation 70% - 10%\n",
        "print(X_train.shape)\n",
        "print(X_val.shape)\n",
        "print()\n",
        "\n",
        "# classi\n",
        "y = [x[0:4] for x in df1['cpv']]\n",
        "X_main, X_test, y_main, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y) # divisione in main e test 80% - 20%\n",
        "print(X_main.shape)\n",
        "print(X_test.shape)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_main, y_main, test_size=0.125, random_state=42, stratify=y_main) # divisione del main in train e valutation 70% - 10%\n",
        "print(X_train.shape)\n",
        "print(X_val.shape)\n",
        "print()\n",
        "\n",
        "# categorie\n",
        "y = [x[0:5] for x in df1['cpv']]\n",
        "X_main, X_test, y_main, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y) # divisione in main e test 80% - 20%\n",
        "print(X_main.shape)\n",
        "print(X_test.shape)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_main, y_main, test_size=0.125, random_state=42, stratify=y_main) # divisione del main in train e valutation 70% - 10%\n",
        "print(X_train.shape)\n",
        "print(X_val.shape)\n",
        "print()\n",
        "\n",
        "# cpv\n",
        "y = [x[0:10] for x in df1['cpv']]\n",
        "X_main, X_test, y_main, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y) # divisione in main e test 80% - 20%\n",
        "print(X_main.shape)\n",
        "print(X_test.shape)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_main, y_main, test_size=0.125, random_state=42, stratify=y_main) # divisione del main in train e valutation 70% - 10%\n",
        "print(X_train.shape)\n",
        "print(X_val.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8OFE3_ht-0Il"
      },
      "outputs": [],
      "source": [
        "skf = StratifiedKFold(y, n_folds=2) #2-fold cross validation\n",
        "len(skf)\n",
        "for train_index, test_index in skf:\n",
        "  print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
        "  X_train, X_test = X[train_index], X[test_index]\n",
        "  y_train, y_test = y[train_index], y[test_index]\n",
        "#fit and predict with X_train/test. Use accuracy metrics to check validation performance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DLX5kAu6iR7s"
      },
      "outputs": [],
      "source": [
        "# transform labels\n",
        "i = 0\n",
        "while i<len(y):\n",
        "  y[i] = y[i][0:2] # divisioni\n",
        "  #y[i] = y[i][0:3] # gruppi\n",
        "  #y[i] = y[i][0:4] # classi\n",
        "  #y[i] = y[i][0:5] # categorie\n",
        "  i = i + 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DxASFq5shS9h"
      },
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JQhS9B4dxR3S"
      },
      "outputs": [],
      "source": [
        "# Linear Support Vector Machine classifier\n",
        "classifier = svm.LinearSVC()\n",
        "# Multi-layer Perceptron classifier\n",
        "#classifier = neural_network.MLPClassifier()\n",
        "# Create the pipeline\n",
        "pipe = Pipeline([(\"cleaner\", predictors()),\n",
        "('vectorizer', vector),\n",
        "('classifier', classifier)])\n",
        "# Fit the model\n",
        "pipe.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Mf2MoMoy6A8"
      },
      "outputs": [],
      "source": [
        "predicted = pipe.predict(X_test)\n",
        "acc = metrics.accuracy_score(y_test, predicted)\n",
        "print(\"Accuracy:\", acc)\n",
        "precision = metrics.precision_score(y_test, predicted, average='macro')\n",
        "recall = metrics.recall_score(y_test, predicted, average='macro')\n",
        "fm = metrics.f1_score(y_test, predicted, average='macro')\n",
        "print(\"P={0}, R={1}, F1={2}\".format(precision, recall, fm))\n",
        "print(\"======================================================\")\n",
        "print(metrics.classification_report(y_test, predicted, digits=4))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RsMEE9BRETpQ"
      },
      "outputs": [],
      "source": [
        "kf = KFold(n_splits=5, random_state=42, shuffle=True)\n",
        "fmicro = 0\n",
        "fmacro = 0\n",
        "for train_index, test_index in kf.split(X):\n",
        "  pipe.fit(X[train_index], y[train_index])\n",
        "  predicted = pipe.predict(X[test_index])\n",
        "  fmacro = fmacro + metrics.f1_score(y[test_index], predicted, average='macro')\n",
        "  fmicro = fmicro + metrics.f1_score(y[test_index], predicted, average='micro')\n",
        "print(\"F-macro: \",fmacro/5)\n",
        "print(\"F-micro: \",fmicro/5)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}